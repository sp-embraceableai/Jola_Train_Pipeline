# Configuration for embraceableAI/OLMo-2-1124-13B-Instruct-16k-Context-FT-ChatML
# Optimized for CPU training (slower but compatible)

model_config:
  pretrained_model_name_or_path: "embraceableAI/OLMo-2-1124-13B-Instruct-16k-Context-FT-ChatML"
  device: "cpu"
  cache_dir: null  # Use default cache directory
  applied_module: 'attention'
  base_model_name: 'olmo2_13b_instruct_16k'

training_config:
  # Learning Rate & Optimization
  learning_rate: 0.0001  # Lower LR for 13B model
  lr_scheduler_type: 'cosine'
  warmup_steps: 50  # Reduced for CPU
  weight_decay: 0.01
  
  # Batch Sizes (optimized for CPU)
  per_device_train_batch_size: 1  # Much smaller for CPU
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 8  # Compensate with more accumulation
  
  # Training Duration
  num_train_epochs: 3  # Reduced for CPU testing
  max_steps: null  # Use epochs instead
  
  # Evaluation & Saving
  evaluation_strategy: 'epoch'
  save_strategy: 'epoch'
  load_best_model_at_end: false  # Disable to save memory
  save_total_limit: 1  # Keep only latest checkpoint
  metric_for_best_model: 'eval_loss'
  greater_is_better: false
  
  # Logging & Monitoring
  report_to: null  # Disable W&B for CPU training
  logging_strategy: "steps"
  logging_steps: 50  # Less frequent logging
  logging_first_step: true
  
  # Reproducibility
  seed: 42
  dataloader_pin_memory: false  # Disable for CPU
  
  # Training Mode
  do_train: true
  do_eval: true
  do_predict: false
  
  # Precision & Memory Optimization
  bf16: false  # Not supported on most CPUs
  fp16: false  # Not supported on most CPUs
  dataloader_num_workers: 0  # Single thread for CPU
  
  # Gradient Clipping
  max_grad_norm: 1.0
  
  # Output
  output_dir: './outputs/olmo2_13b_instruct_16k_cpu'
  overwrite_output_dir: true
  
  # Checkpointing
  save_steps: 1000  # Less frequent saves
  eval_steps: 1000
  
  # Early Stopping
  early_stopping_patience: 2
  early_stopping_threshold: 0.001

data_config:
  train_size: 100  # Much smaller dataset for CPU
  valid_size: 20
  test_size: 20
  task_name: "sharegpt"
  data_path: "data/sharegpt_training_data_combined_final_solution_correctness_100.jsonl"
  format_type: "chatml"
  max_length: 1024  # Much shorter sequences for CPU
  split_ratio: 0.8

jola_config:
  gate_lambda: 0.00004
  gate_scheduler: "expon"
  gate_warmup_steps: 50

# Hardware Configuration
hardware_config:
  device_map: null  # No device mapping for CPU
  torch_dtype: "float32"  # Use float32 for CPU
  use_flash_attention: false  # Not available on CPU
  use_gradient_checkpointing: true  # Still useful for memory
  
# Logging Configuration
logging_config:
  project_name: "jola-olmo2-13b-cpu-training"
  run_name: "olmo2-13b-instruct-16k-sharegpt-cpu"
  tags: ["olmo2", "13b", "jola", "sharegpt", "cpu"]
